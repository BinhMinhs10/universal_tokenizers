# Universal tokenizers

* Build wordpiece vocab all file .txt and .tok
```bash
python build_wordpiece_vocab.py \
    --corpus_dir data \
    --output_dir outputs
```

* Build SentencePiece vocab
```bash
```
